{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# def func(frame):    \n",
    "#     frame = cv2.resize(frame,(128,128))\n",
    "#     converted2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "# #     cv2.imshow(\"GRAY\", frame)\n",
    "# #     cv2.waitKey(0)\n",
    "#     converted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # Convert from RGB to HSV\n",
    "#     #cv2.imshow(\"original\",converted2)\n",
    "# #     cv2.imshow(\"HSV\", converted)\n",
    "# #     cv2.waitKey(0)\n",
    "#     lowerBoundary = np.array([0,40,30],dtype=\"uint8\")\n",
    "#     upperBoundary = np.array([43,255,254],dtype=\"uint8\")\n",
    "#     skinMask = cv2.inRange(converted, lowerBoundary, upperBoundary)\n",
    "#     skinMask = cv2.addWeighted(skinMask,0.5,skinMask,0.5,0.0)\n",
    "#     #cv2.imshow(\"masked\",skinMask)\n",
    "# #     cv2.imshow(\"masked\",skinMask)\n",
    "# #     cv2.waitKey(0)\n",
    "#     skinMask = cv2.medianBlur(skinMask, 5)\n",
    "    \n",
    "#     skin = cv2.bitwise_and(converted2, converted2, mask = skinMask)\n",
    "#     #frame = cv2.addWeighted(frame,1.5,skin,-0.5,0)\n",
    "#     #skin = cv2.bitwise_and(frame, frame, mask = skinMask)\n",
    "\n",
    "#     #skinGray=cv2.cvtColor(skin, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "# #     cv2.imshow(\"masked2\",skin)\n",
    "# #     cv2.waitKey(0)\n",
    "#     img2 = cv2.Canny(skin,60,60)\n",
    "# #     cv2.imshow(\"edge detection\",img2)\n",
    "# #     cv2.waitKey(0)\n",
    "#     #\n",
    "    \n",
    "#     ''' \n",
    "#     hog = cv2.HOGDescriptor()\n",
    "#     h = hog.compute(img2)\n",
    "#     print(len(h))\n",
    "    \n",
    "#     '''\n",
    "#     surf = cv2.xfeatures2d.SURF_create()\n",
    "#     #surf.extended=True\n",
    "#     img2 = cv2.resize(img2,(256,256))\n",
    "# #     cv2.imshow(\"edge detection 256 \",img2)\n",
    "# #     cv2.waitKey(0)\n",
    "#     kp, des = surf.detectAndCompute(img2,None)\n",
    "#     #print(len(des))\n",
    "#     img2 = cv2.drawKeypoints(img2,kp,None,(0,0,255),4)\n",
    "# #     cv2.imshow(\"SURF Features \",img2)\n",
    "# #     cv2.waitKey(0)\n",
    "# #     plt.imshow(img2),plt.show()\n",
    "    \n",
    "# #     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "#     #print(len(des))\n",
    "#     return des\n",
    "\n",
    "# def func2(path):    \n",
    "#     frame = cv2.imread(path)\n",
    "#     frame = cv2.resize(frame,(128,128))\n",
    "#     converted2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     converted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # Convert from RGB to HSV\n",
    "#     #cv2.imshow(\"original\",converted2)\n",
    "\n",
    "#     lowerBoundary = np.array([0,40,30],dtype=\"uint8\")\n",
    "#     upperBoundary = np.array([43,255,254],dtype=\"uint8\")\n",
    "#     skinMask = cv2.inRange(converted, lowerBoundary, upperBoundary)\n",
    "#     skinMask = cv2.addWeighted(skinMask,0.5,skinMask,0.5,0.0)\n",
    "#     #cv2.imshow(\"masked\",skinMask)\n",
    "    \n",
    "#     skinMask = cv2.medianBlur(skinMask, 5)\n",
    "    \n",
    "#     skin = cv2.bitwise_and(converted2, converted2, mask = skinMask)\n",
    "    \n",
    "#     #cv2.imshow(\"masked2\",skin)\n",
    "#     img2 = cv2.Canny(skin,60,60)\n",
    "#     #cv2.imshow(\"edge detection\",img2)\n",
    "#     img2 = cv2.resize(img2,(256,256))\n",
    "#     orb = cv2.xfeatures2d.ORB_create()\n",
    "#     kp, des = orb.detectAndCompute(img2,None)\n",
    "\n",
    "#     #print(len(des2))\n",
    "#     img2 = cv2.drawKeypoints(img2,kp,None,color=(0,255,0), flags=0)\n",
    "#     #plt.imshow(img2),plt.show()\n",
    "    \n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "#     return des2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "def make_background_black(frame):\n",
    "    \"\"\"0\n",
    "    Makes everything apart from the main object of interest to be black in color.\n",
    "    \"\"\"\n",
    "    #print(\"Making background black...\")\n",
    "\n",
    "    # Convert from RGB to HSV\n",
    "    #print(frame.shape)\n",
    "    #print(frame)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    #print(frame.shape)\n",
    "    #print(frame)\n",
    "    # Prepare the first mask.\n",
    "    # Tuned parameters to match the skin color of the input images...\n",
    "    #print(frame.dtype)\n",
    "    #cv2.imshow(\"mask1\",  np.array[0,40,30])\n",
    "    #cv2.imshow(\"mask2\",  np.array[43,255,254])\n",
    "    lower_boundary = np.array([0, 40, 30], dtype=\"uint8\")\n",
    "    upper_boundary = np.array([43, 255, 254], dtype=\"uint8\")\n",
    "    skin_mask = cv2.inRange(frame, lower_boundary, upper_boundary)\n",
    "#     Pixels that are white (255) in the mask represent areas of the frame that are skin. \n",
    "#     Pixels that are black (0) in the mask represent areas that are not skin.\n",
    "\n",
    "\n",
    "    # Apply a series of erosions and dilations to the mask using an\n",
    "    # elliptical kernel\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    skin_mask = cv2.erode(skin_mask, kernel, iterations=2)\n",
    "    skin_mask = cv2.dilate(skin_mask, kernel, iterations=2)\n",
    "    \n",
    "#     we may detect many small false-positive skin regions in the image. \n",
    "#     To remove these small regions,we create an elliptical structuring kernel. \n",
    "#     Then, we use this kernel to perform two iterations of erosions and dilations, respectively. \n",
    "#     These erosions and dilations will help remove the small false-positive skin regions in the image.\n",
    "    # Prepare the second mask\n",
    "    lower_boundary = np.array([170, 80, 30], dtype=\"uint8\")\n",
    "    upper_boundary = np.array([180, 255, 250], dtype=\"uint8\")\n",
    "    skin_mask2 = cv2.inRange(frame, lower_boundary, upper_boundary)\n",
    "\n",
    "    # Combine the effect of both the masks to create the final frame.\n",
    "    skin_mask = cv2.addWeighted(skin_mask, 0.5, skin_mask2, 0.5, 0.0)\n",
    "    # Blur the mask to help remove noise.\n",
    "    # skin_mask = cv2.medianBlur(skin_mask, 5)\n",
    "    frame_skin = cv2.bitwise_and(frame, frame, mask=skin_mask)\n",
    "    frame = cv2.addWeighted(frame, 1.5, frame_skin, -0.5, 0)\n",
    "    frame_skin = cv2.bitwise_and(frame, frame, mask=skin_mask)\n",
    "\n",
    "    #print(\"Done!\")\n",
    "    return frame_skin\n",
    "def find_largest_contour_index(contours):\n",
    "    \"\"\"\n",
    "    Finds and returns the index of the largest contour from a list of contours.\n",
    "    Returs `None` if the contour list is empty.\n",
    "    \"\"\"\n",
    "    if len(contours) <= 0:\n",
    "        log_message = \"The length of contour lists is non-positive!\"\n",
    "        raise Exception(log_message)\n",
    "\n",
    "    largest_contour_index = 0\n",
    "\n",
    "    contour_iterator = 1\n",
    "    while contour_iterator < len(contours):\n",
    "        if cv2.contourArea(contours[contour_iterator]) > cv2.contourArea(contours[largest_contour_index]):\n",
    "            largest_contour_index = contour_iterator\n",
    "        contour_iterator += 1\n",
    "\n",
    "    return largest_contour_index\n",
    "\n",
    "\n",
    "def draw_contours(frame):\n",
    "    \"\"\"\n",
    "    Draws a contour around white color.\n",
    "    \"\"\"\n",
    "    #print(\"Drawing contour around white color...\")\n",
    "\n",
    "    # 'contours' is a list of contours found.\n",
    "    _, contours, _ = cv2.findContours(\n",
    "        frame, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    #print(\"No of contours: \"+str(len(contours)))\n",
    "    cv2.drawContours(frame,contours,0,(255, 255, 255), thickness=-1)\n",
    "    #cv2.imshow('Display5',frame)\n",
    "    #cv2.waitKey(0)\n",
    "#     cv2.drawContours(frame,contours,1,(0,255,0), thickness=-1)\n",
    "    \n",
    "#     #cv2.namedWindow('Display',cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('Display1',frame)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.drawContours(frame,contours,2,(0,255, 0), thickness=2)\n",
    "    \n",
    "#     #cv2.namedWindow('Display',cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('Display2',frame)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.drawContours(frame,contours,3,(0, 0, 255), thickness=-1)\n",
    "    \n",
    "#     #cv2.namedWindow('Display',cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('Display3',frame)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.drawContours(frame,contours,4,(0, 255, 255), thickness=-1)\n",
    "    \n",
    "#     #cv2.namedWindow('Display',cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow('Display4',frame)\n",
    "#     cv2.waitKey(0)\n",
    "    \n",
    "#     \n",
    "#     cv2.drawContours(frame,contours,largest_contour_index,(255, 0, 255), thickness=-1)\n",
    "#     cv2.imshow('Display5',frame)\n",
    "#     cv2.waitKey(0)\n",
    "    # Finding the contour with the greatest area.\n",
    "    largest_contour_index = find_largest_contour_index(contours)\n",
    "    #print(\"Largest Contour index\"+str(largest_contour_index))\n",
    "    # Draw the largest contour in the image.\n",
    "    cv2.drawContours(frame, contours,\n",
    "                     largest_contour_index, (255, 255, 255), thickness=-1)\n",
    "#     cv2.imshow('Display5',frame)\n",
    "#     cv2.waitKey(0)\n",
    "    # Draw a rectangle around the contour perimeter\n",
    "    contour_dimensions = cv2.boundingRect(contours[largest_contour_index])\n",
    "    \n",
    "    #x,y,w,h = cv2.boundingRect(cnt)\n",
    "    #(x,y) be the top-left coordinate of the rectangle and (w,h) be its width and height. \n",
    "    \n",
    "    \n",
    "    # cv2.rectangle(sign_image,(x,y),(x+w,y+h),(255,255,255),0,8)\n",
    "\n",
    "    #print(\"Done!\")\n",
    "    return (frame, contour_dimensions)\n",
    "\n",
    "def func(frame):    \n",
    "    frame = cv2.resize(frame,(128,128))\n",
    "    #converted2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     cv2.imshow(\"GRAY\", frame)\n",
    "#     cv2.waitKey(0)\n",
    "    #skin = cv2.Canny(frame,60,60)\n",
    "    skin=make_background_black(frame)\n",
    "    height, width = skin.shape[:2]\n",
    "\n",
    "    # Convert image from HSV to BGR format\n",
    "    skin = cv2.cvtColor(skin, cv2.COLOR_HSV2BGR)\n",
    "    # Convert image from BGR to gray format\n",
    "    skin = cv2.cvtColor(skin, cv2.COLOR_BGR2GRAY)\n",
    "    # Highlight the main object, remove noise \n",
    "    skin = cv2.GaussianBlur(skin, (5, 5), 0)\n",
    "\n",
    "    threshold = 1\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if skin[i][j] > threshold:\n",
    "                # Setting the skin tone to be white.\n",
    "                skin[i][j] = 255\n",
    "            else:\n",
    "                # Setting everything else to be black.\n",
    "                skin[i][j] = 0\n",
    "    skin = skin[:height - 15, :]\n",
    "    skin, contour_dimensions = draw_contours(skin)\n",
    "    \n",
    "#     cv2.imshow(\"edge detection\",img2)\n",
    "#     cv2.waitKey(0)\n",
    "    #\n",
    "    \n",
    "    ''' \n",
    "    hog = cv2.HOGDescriptor()\n",
    "    h = hog.compute(img2)\n",
    "    print(len(h))\n",
    "    \n",
    "    '''\n",
    "    skin = cv2.Canny(frame,60,60)\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    #surf.extended=True\n",
    "    img2 = cv2.resize(skin,(256,256))\n",
    "#     cv2.imshow(\"edge detection 256 \",img2)\n",
    "#     cv2.waitKey(0)\n",
    "    kp, des = surf.detectAndCompute(img2,None)\n",
    "    #print(len(des))\n",
    "    img2 = cv2.drawKeypoints(img2,kp,None,(0,0,255),4)\n",
    "#     cv2.imshow(\"SURF Features \",img2)\n",
    "#     cv2.waitKey(0)\n",
    "#     plt.imshow(img2),plt.show()\n",
    "    \n",
    "#     cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    #print(len(des))\n",
    "    return des\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_translate(a):\n",
    "    my_dict = {'a':0,'b':1,'d':2,'e':3,'f':4,'g':5,'h':6,'j':7,'k':8,'m':9,\n",
    "               'n':10,'o':11,'p':12,'q':13,'r':14,'s':15,'t':16,'x':17,'y':18,\n",
    "               'z':19}\n",
    "    return np.vectorize(my_dict.__getitem__)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test-val split: 461 training rows, 306 test rows, 0 validation rows\n",
      "124339 descriptors before clustering\n",
      "Clustering on training set to get codebook of 150 words\n",
      "done clustering. Using clustering model to generate BoW histograms for each image.\n",
      "done generating BoW histograms.\n",
      "knn started\n",
      "accuracy score for  K nearest neighbours 0.928104575163\n",
      "precision_score for  K nearest neighbours 0.928104575163\n",
      "f1 score for  K nearest neighbours 0.928104575163\n",
      "recall score for  K nearest neighbours 0.928104575163\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "svm started\n",
      "accuracy score for  SVM 0.967320261438\n",
      "precision_score for  SVM 0.967320261438\n",
      "f1 score for  SVM 0.967320261438\n",
      "recall score for  SVM 0.967320261438\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "lr started\n",
      "accuracy score for  Logistic regression 0.950980392157\n",
      "precision_score for  Logistic regression 0.950980392157\n",
      "f1 score for  Logistic regression 0.950980392157\n",
      "recall score for  Logistic regression 0.950980392157\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "nb started\n",
      "accuracy score for  Naive Bayes 0.803921568627\n",
      "precision_score for  Naive Bayes 0.803921568627\n",
      "f1 score for  Naive Bayes 0.803921568627\n",
      "recall score for  Naive Bayes 0.803921568627\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "mlp started\n",
      "accuracy score for  MLP classifier 0.93137254902\n",
      "precision_score for  MLP classifier 0.93137254902\n",
      "f1 score for  MLP classifier 0.93137254902\n",
      "recall score for  MLP classifier 0.93137254902\n",
      "------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import random\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.naive_bayes import GaussianNB as nb\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "import numpy as np\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.externals import joblib\n",
    "from common.config import get_config\n",
    "from sklearn import metrics\n",
    "from common.image_transformation import apply_image_transformation\n",
    "#initialise\n",
    "label=0\n",
    "img_descs=[]\n",
    "y=[]\n",
    "\n",
    "#utility functions\n",
    "def perform_data_split(X, y, training_idxs, test_idxs, val_idxs):\n",
    "    \"\"\"\n",
    "    Split X and y into train/test/val sets\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : eg, use img_bow_hist\n",
    "    y : corresponding labels for X\n",
    "    training_idxs : list/array of integers used as indicies for training rows\n",
    "    test_idxs : same\n",
    "    val_idxs : same\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    \"\"\"\n",
    "    X_train = X[training_idxs]\n",
    "    X_test = X[test_idxs]\n",
    "    X_val = X[val_idxs]\n",
    "\n",
    "    y_train = y[training_idxs]\n",
    "    y_test = y[test_idxs]\n",
    "    y_val = y[val_idxs]\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "def train_test_val_split_idxs(total_rows, percent_test, percent_val):\n",
    "    \"\"\"\n",
    "    Get indexes for training, test, and validation rows, given a total number of rows.\n",
    "    Assumes indexes are sequential integers starting at 0: eg [0,1,2,3,...N]\n",
    "    Returns:\n",
    "    --------\n",
    "    training_idxs, test_idxs, val_idxs\n",
    "        Both lists of integers\n",
    "    \"\"\"\n",
    "    if percent_test + percent_val >= 1.0:\n",
    "        raise ValueError('percent_test and percent_val must sum to less than 1.0')\n",
    "\n",
    "    row_range = range(total_rows)\n",
    "\n",
    "    no_test_rows = int(total_rows*(percent_test))\n",
    "    test_idxs = np.random.choice(row_range, size=no_test_rows, replace=False)\n",
    "    # remove test indexes\n",
    "    row_range = [idx for idx in row_range if idx not in test_idxs]\n",
    "\n",
    "    no_val_rows = int(total_rows*(percent_val))\n",
    "    val_idxs = np.random.choice(row_range, size=no_val_rows, replace=False)\n",
    "    # remove validation indexes\n",
    "    training_idxs = [idx for idx in row_range if idx not in val_idxs]\n",
    "\n",
    "    print('Train-test-val split: %i training rows, %i test rows, %i validation rows' % (len(training_idxs), len(test_idxs), len(val_idxs)))\n",
    "\n",
    "    return training_idxs, test_idxs, val_idxs\n",
    "def print_with_precision(num):\n",
    "    return \"%0.5f\" % num\n",
    "\n",
    "def cluster_features(img_descs, training_idxs, cluster_model):\n",
    "    \"\"\"\n",
    "    Cluster the training features using the cluster_model\n",
    "    and convert each set of descriptors in img_descs\n",
    "    to a Visual Bag of Words histogram.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : list of lists of SIFT descriptors (img_descs)\n",
    "    training_idxs : array/list of integers\n",
    "        Indicies for the training rows in img_descs\n",
    "    cluster_model : clustering model (eg KMeans from scikit-learn)\n",
    "        The model used to cluster the SIFT features\n",
    "    Returns:\n",
    "    --------\n",
    "    X, cluster_model :\n",
    "        X has K feature columns, each column corresponding to a visual word\n",
    "        cluster_model has been fit to the training set\n",
    "    \"\"\"\n",
    "    n_clusters = cluster_model.n_clusters\n",
    "\n",
    "    # # Generate the SIFT descriptor features\n",
    "    # img_descs = gen_sift_features(labeled_img_paths)\n",
    "    #\n",
    "    # # Generate indexes of training rows\n",
    "    # total_rows = len(img_descs)\n",
    "    # training_idxs, test_idxs, val_idxs = train_test_val_split_idxs(total_rows, percent_test, percent_val)\n",
    "\n",
    "    # Concatenate all descriptors in the training set together\n",
    "    training_descs = [img_descs[i] for i in training_idxs]\n",
    "    all_train_descriptors = [desc for desc_list in training_descs for desc in desc_list]\n",
    "    all_train_descriptors = np.array(all_train_descriptors)\n",
    "\n",
    "\n",
    "    print ('%i descriptors before clustering' % all_train_descriptors.shape[0])\n",
    "\n",
    "    # Cluster descriptors to get codebook\n",
    "    #print ('Using clustering model %s...' % repr(cluster_model))\n",
    "    print ('Clustering on training set to get codebook of %i words' % n_clusters)\n",
    "\n",
    "    # train kmeans or other cluster model on those descriptors selected above\n",
    "    cluster_model.fit(all_train_descriptors)\n",
    "    print ('done clustering. Using clustering model to generate BoW histograms for each image.')\n",
    "\n",
    "    # compute set of cluster-reduced words for each image\n",
    "    img_clustered_words = [cluster_model.predict(raw_words) for raw_words in img_descs]\n",
    "\n",
    "    # finally make a histogram of clustered word counts for each image. These are the final features.\n",
    "    img_bow_hist = np.array(\n",
    "        [np.bincount(clustered_words, minlength=n_clusters) for clustered_words in img_clustered_words])\n",
    "\n",
    "    X = img_bow_hist\n",
    "    print ('done generating BoW histograms.')\n",
    "\n",
    "    return X, cluster_model\n",
    "\n",
    "def calc_accuracy(method,label_test,pred):\n",
    "    print(\"accuracy score for \",method,sm.accuracy_score(label_test,pred))\n",
    "    print(\"precision_score for \",method,sm.precision_score(label_test,pred,average='micro'))\n",
    "    print(\"f1 score for \",method,sm.f1_score(label_test,pred,average='micro'))\n",
    "    print(\"recall score for \",method,sm.recall_score(label_test,pred,average='micro'))\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "def predict_svm(X_train, X_test, y_train, y_test):\n",
    "    svc=SVC(kernel='linear') \n",
    "    print(\"svm started\")\n",
    "    svc.fit(X_train,y_train)\n",
    "    y_pred=svc.predict(X_test)\n",
    "    calc_accuracy(\"SVM\",y_test,y_pred)\n",
    "    np.savetxt('submission_surf_svm.csv', np.c_[range(1,len(y_test)+1),y_pred,y_test], delimiter=',', header = 'ImageId,Label,TrueLabel', comments = '', fmt='%d')\n",
    "    with open(r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\SVM\\stats-svm.txt', \"w\") as model_stats_file:\n",
    "        model_stats_file.write(\"Model used = Gaussian Naive Bayes\")\n",
    "        model_stats_file.write(\"Classifier model details:\\n{}\\n\\n\".format(svc))\n",
    "        joblib.dump(svc,r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\SVM\\svm.pkl')\n",
    "        score = sm.accuracy_score(y_test,y_pred)\n",
    "        model_stats_file.write(\"Model score:\\n{}\\n\\n\".format(print_with_precision(score)))\n",
    "        report = metrics.classification_report(y_test,y_pred)\n",
    "        model_stats_file.write(\"Classification report:\\n{}\\n\\n\".format(report))\n",
    "\n",
    "def predict_lr(X_train, X_test, y_train, y_test):\n",
    "    clf = lr()\n",
    "    print(\"lr started\")\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    calc_accuracy(\"Logistic regression\",y_test,y_pred)\n",
    "    np.savetxt('submission_surf_lr.csv', np.c_[range(1,len(y_test)+1),y_pred,y_test], delimiter=',', header = 'ImageId,Label,TrueLabel', comments = '', fmt='%d')\n",
    "    with open(r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\Logistic\\stats-lr.txt', \"w\") as model_stats_file:\n",
    "        model_stats_file.write(\"Model used = Gaussian Naive Bayes\")\n",
    "        model_stats_file.write(\"Classifier model details:\\n{}\\n\\n\".format(clf))\n",
    "        joblib.dump(clf,r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\Logistic\\lr.pkl')\n",
    "        score = sm.accuracy_score(y_test,y_pred)\n",
    "        model_stats_file.write(\"Model score:\\n{}\\n\\n\".format(print_with_precision(score)))\n",
    "        report = metrics.classification_report(y_test,y_pred)\n",
    "        model_stats_file.write(\"Classification report:\\n{}\\n\\n\".format(report))\n",
    "\n",
    "\n",
    "def predict_nb(X_train, X_test, y_train, y_test):\n",
    "    clf = nb()\n",
    "    print(\"nb started\")\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    calc_accuracy(\"Naive Bayes\",y_test,y_pred)\n",
    "    np.savetxt('submission_surf_nb.csv', np.c_[range(1,len(y_test)+1),y_pred,y_test], delimiter=',', header = 'ImageId,Label,TrueLabel', comments = '', fmt='%d')\n",
    "    with open(r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\Gaussian_NB\\stats-nb.txt', \"w\") as model_stats_file:\n",
    "        model_stats_file.write(\"Model used = Gaussian Naive Bayes\")\n",
    "        model_stats_file.write(\"Classifier model details:\\n{}\\n\\n\".format(clf))\n",
    "        joblib.dump(clf,r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\Gaussian_NB\\nb.pkl')\n",
    "        score = sm.accuracy_score(y_test,y_pred)\n",
    "        model_stats_file.write(\"Model score:\\n{}\\n\\n\".format(print_with_precision(score)))\n",
    "        report = metrics.classification_report(y_test,y_pred)\n",
    "        model_stats_file.write(\"Classification report:\\n{}\\n\\n\".format(report))\n",
    "\n",
    "def predict_knn(X_train, X_test, y_train, y_test):\n",
    "    clf=knn(n_neighbors=3)\n",
    "    print(\"knn started\")\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    calc_accuracy(\"K nearest neighbours\",y_test,y_pred)\n",
    "    np.savetxt(r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\KNN\\submission_surf_knn.csv', np.c_[range(1,len(y_test)+1),y_pred,y_test], delimiter=',', header = 'ImageId,Label,TrueLabel', comments = '', fmt='%d')\n",
    "    with open(r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\KNN\\stats-knn.txt', \"w\") as model_stats_file:\n",
    "        model_stats_file.write(\"Model used = Gaussian Naive Bayes\")\n",
    "        model_stats_file.write(\"Classifier model details:\\n{}\\n\\n\".format(clf))\n",
    "        joblib.dump(clf,r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\KNN\\knn.pkl')\n",
    "        score = sm.accuracy_score(y_test,y_pred)\n",
    "        model_stats_file.write(\"Model score:\\n{}\\n\\n\".format(print_with_precision(score)))\n",
    "        report = metrics.classification_report(y_test,y_pred)\n",
    "        model_stats_file.write(\"Classification report:\\n{}\\n\\n\".format(report))\n",
    "\n",
    "def predict_mlp(X_train, X_test, y_train, y_test):\n",
    "    clf=mlp()\n",
    "    print(\"mlp started\")\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    calc_accuracy(\"MLP classifier\",y_test,y_pred)\n",
    "    with open(r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\MLP\\stats-mlp.txt', \"w\") as model_stats_file:\n",
    "        model_stats_file.write(\"Model used = Gaussian Naive Bayes\")\n",
    "        model_stats_file.write(\"Classifier model details:\\n{}\\n\\n\".format(clf))\n",
    "        joblib.dump(clf,r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\MLP\\mlp.pkl')\n",
    "        score = sm.accuracy_score(y_test,y_pred)\n",
    "        model_stats_file.write(\"Model score:\\n{}\\n\\n\".format(print_with_precision(score)))\n",
    "        report = metrics.classification_report(y_test,y_pred)\n",
    "        model_stats_file.write(\"Classification report:\\n{}\\n\\n\".format(report))\n",
    "        \n",
    "training_images_labels_path = get_config('training_images_labels_path')\n",
    "with open(training_images_labels_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    image_path, image_label = line.split()\n",
    "    frame = cv2.imread(image_path)\n",
    "    des=func(frame)\n",
    "    img_descs.append(des)\n",
    "    y.append(image_label)\n",
    "y= vec_translate(y)\n",
    "#finding indexes of test train and validate\n",
    "y=np.array(y)\n",
    "training_idxs, test_idxs, val_idxs = train_test_val_split_idxs(len(img_descs), 0.4, 0.0)\n",
    "\n",
    "#creating histogram using kmeans minibatch cluster model\n",
    "X, cluster_model = cluster_features(img_descs, training_idxs, MiniBatchKMeans(n_clusters=150))\n",
    "\n",
    "#splitting data into test, train, validate using the indexes\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = perform_data_split(X, y, training_idxs, test_idxs, val_idxs)\n",
    "\n",
    "\n",
    "#using classification methods\n",
    "predict_knn(X_train, X_test,y_train, y_test)\n",
    "#predict_mlp(X_train, X_test,y_train, y_test)\n",
    "predict_svm(X_train, X_test,y_train, y_test)\n",
    "\n",
    "predict_lr(X_train, X_test,y_train, y_test)\n",
    "predict_nb(X_train, X_test,y_train, y_test)\n",
    "\n",
    "predict_mlp(X_train, X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test-val split: 270 training rows, 180 test rows, 0 validation rows\n",
      "103245 descriptors before clustering\n",
      "Clustering on training set to get codebook of 150 words\n",
      "done clustering. Using clustering model to generate BoW histograms for each image.\n",
      "done generating BoW histograms.\n",
      "knn started\n",
      "accuracy score for  K nearest neighbours 0.955555555556\n",
      "precision_score for  K nearest neighbours 0.955555555556\n",
      "f1 score for  K nearest neighbours 0.955555555556\n",
      "recall score for  K nearest neighbours 0.955555555556\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "svm started\n",
      "accuracy score for  SVM 0.916666666667\n",
      "precision_score for  SVM 0.916666666667\n",
      "f1 score for  SVM 0.916666666667\n",
      "recall score for  SVM 0.916666666667\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "lr started\n",
      "accuracy score for  Logistic regression 0.938888888889\n",
      "precision_score for  Logistic regression 0.938888888889\n",
      "f1 score for  Logistic regression 0.938888888889\n",
      "recall score for  Logistic regression 0.938888888889\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "nb started\n",
      "accuracy score for  Naive Bayes 0.8\n",
      "precision_score for  Naive Bayes 0.8\n",
      "f1 score for  Naive Bayes 0.8\n",
      "recall score for  Naive Bayes 0.8\n",
      "------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "label=0\n",
    "img_descs=[]\n",
    "y=[]\n",
    "testing_images_labels_path = get_config('testing_images_labels_path')\n",
    "with open(testing_images_labels_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    image_path, image_label = line.split()\n",
    "    frame = cv2.imread(image_path)\n",
    "    des=func(frame)\n",
    "    img_descs.append(des)\n",
    "    y.append(image_label)\n",
    "y= vec_translate(y)\n",
    "#finding indexes of test train and validate\n",
    "y=np.array(y)\n",
    "training_idxs, test_idxs, val_idxs = train_test_val_split_idxs(len(img_descs), 0.4, 0.0)\n",
    "\n",
    "#creating histogram using kmeans minibatch cluster model\n",
    "X, cluster_model = cluster_features(img_descs, training_idxs, MiniBatchKMeans(n_clusters=150))\n",
    "\n",
    "#splitting data into test, train, validate using the indexes\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = perform_data_split(X, y, training_idxs, test_idxs, val_idxs)\n",
    "\n",
    "\n",
    "#using classification methods\n",
    "predict_knn(X_train, X_test,y_train, y_test)\n",
    "#predict_mlp(X_train, X_test,y_train, y_test)\n",
    "predict_svm(X_train, X_test,y_train, y_test)\n",
    "\n",
    "predict_lr(X_train, X_test,y_train, y_test)\n",
    "predict_nb(X_train, X_test,y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172773 descriptors before clustering\n",
      "Clustering on training set to get codebook of 150 words\n",
      "done clustering. Using clustering model to generate BoW histograms for each image.\n",
      "done generating BoW histograms.\n",
      "432 450\n",
      "Accuracy of Gaussain NB Classification on Test Data is: 0.04\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from common.config import get_config\n",
    "from common.image_transformation import apply_image_transformation\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    label=0\n",
    "    img_descs=[]\n",
    "    y=[]\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model_serialized_path=r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\Gaussian_NB\\nb.pkl'\n",
    "    #print(\"Using model '{}'...\".format(model_name))\n",
    "    testing_images_labels_path = get_config('testing_images_labels_path')\n",
    "    with open(testing_images_labels_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        image_path, image_label = line.split()\n",
    "        frame = cv2.imread(image_path)\n",
    "        des=func(frame)\n",
    "        img_descs.append(des)\n",
    "        y.append(image_label)\n",
    "    y= vec_translate(y)\n",
    "    #finding indexes of test train and validate\n",
    "    y=np.array(y)\n",
    "    idxs=[]\n",
    "    for i in range(len(img_descs)):\n",
    "        idxs.append(i)\n",
    "    X, cluster_model = cluster_features(img_descs,idxs, MiniBatchKMeans(n_clusters=150))\n",
    "    \n",
    "    \n",
    "    total=len(img_descs)\n",
    "    cnt=0\n",
    "    classifier_model = joblib.load(model_serialized_path)\n",
    "    predicted_labels = classifier_model.predict(X)\n",
    "#     predicted_label = predicted_labels[0]\n",
    "    #print(predicted_labels)\n",
    "    for i in range(total):\n",
    "        if y[i] != predicted_labels[i]:\n",
    "                    cnt += 1\n",
    "    print(str(cnt)+\" \"+str(total))\n",
    "    print(\"Accuracy of Gaussain NB Classification on Test Data is: \"+str((total-cnt)/total))\n",
    "    cv2.destroyAllWindows()\n",
    "    #print (\"The program completed successfully !!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172773 descriptors before clustering\n",
      "Clustering on training set to get codebook of 150 words\n",
      "done clustering. Using clustering model to generate BoW histograms for each image.\n",
      "done generating BoW histograms.\n",
      "440 450\n",
      "Accuracy of KNN Classification on Test Data is: 0.022222222222222223\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from common.config import get_config\n",
    "from common.image_transformation import apply_image_transformation\n",
    "import warnings\n",
    "\n",
    "def main():\n",
    "    label=0\n",
    "    img_descs=[]\n",
    "    y=[]\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model_serialized_path=r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\KNN\\knn.pkl'\n",
    "    #print(\"Using model '{}'...\".format(model_name))\n",
    "    testing_images_labels_path = get_config('testing_images_labels_path')\n",
    "    with open(testing_images_labels_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        image_path, image_label = line.split()\n",
    "        frame = cv2.imread(image_path)\n",
    "        des=func(frame)\n",
    "        img_descs.append(des)\n",
    "        y.append(image_label)\n",
    "    y= vec_translate(y)\n",
    "    #finding indexes of test train and validate\n",
    "    y=np.array(y)\n",
    "    idxs=[]\n",
    "    for i in range(len(img_descs)):\n",
    "        idxs.append(i)\n",
    "    X, cluster_model = cluster_features(img_descs,idxs, MiniBatchKMeans(n_clusters=150))\n",
    "    \n",
    "    \n",
    "    total=len(img_descs)\n",
    "    cnt=0\n",
    "    classifier_model = joblib.load(model_serialized_path)\n",
    "    predicted_labels = classifier_model.predict(X)\n",
    "#     predicted_label = predicted_labels[0]\n",
    "    #print(predicted_labels)\n",
    "    for i in range(total):\n",
    "        if y[i] != predicted_labels[i]:\n",
    "                    cnt += 1\n",
    "    print(str(cnt)+\" \"+str(total))\n",
    "    print(\"Accuracy of KNN Classification on Test Data is: \"+str((total-cnt)/total))\n",
    "    cv2.destroyAllWindows()\n",
    "    #print (\"The program completed successfully !!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172773 descriptors before clustering\n",
      "Clustering on training set to get codebook of 150 words\n",
      "done clustering. Using clustering model to generate BoW histograms for each image.\n",
      "done generating BoW histograms.\n",
      "426 450\n",
      "Accuracy of Logistic Regression Classification on Test Data is: 0.05333333333333334\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from common.config import get_config\n",
    "from common.image_transformation import apply_image_transformation\n",
    "import warnings\n",
    "\n",
    "def main():\n",
    "    label=0\n",
    "    img_descs=[]\n",
    "    y=[]\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model_serialized_path=r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\Logistic\\lr.pkl'\n",
    "    #print(\"Using model '{}'...\".format(model_name))\n",
    "    testing_images_labels_path = get_config('testing_images_labels_path')\n",
    "    with open(testing_images_labels_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        image_path, image_label = line.split()\n",
    "        frame = cv2.imread(image_path)\n",
    "        des=func(frame)\n",
    "        img_descs.append(des)\n",
    "        y.append(image_label)\n",
    "    y= vec_translate(y)\n",
    "    #finding indexes of test train and validate\n",
    "    y=np.array(y)\n",
    "    idxs=[]\n",
    "    for i in range(len(img_descs)):\n",
    "        idxs.append(i)\n",
    "    X, cluster_model = cluster_features(img_descs,idxs, MiniBatchKMeans(n_clusters=150))\n",
    "    \n",
    "    \n",
    "    total=len(img_descs)\n",
    "    cnt=0\n",
    "    classifier_model = joblib.load(model_serialized_path)\n",
    "    predicted_labels = classifier_model.predict(X)\n",
    "#     predicted_label = predicted_labels[0]\n",
    "    #print(predicted_labels)\n",
    "    for i in range(total):\n",
    "        if y[i] != predicted_labels[i]:\n",
    "                    cnt += 1\n",
    "    print(str(cnt)+\" \"+str(total))\n",
    "    print(\"Accuracy of Logistic Regression Classification on Test Data is: \"+str((total-cnt)/total))\n",
    "    cv2.destroyAllWindows()\n",
    "    #print (\"The program completed successfully !!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172773 descriptors before clustering\n",
      "Clustering on training set to get codebook of 150 words\n",
      "done clustering. Using clustering model to generate BoW histograms for each image.\n",
      "done generating BoW histograms.\n",
      "427 450\n",
      "Accuracy of SVM Classification on Test Data is: 0.051111111111111114\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from common.config import get_config\n",
    "from common.image_transformation import apply_image_transformation\n",
    "import warnings\n",
    "\n",
    "def main():\n",
    "    label=0\n",
    "    img_descs=[]\n",
    "    y=[]\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model_serialized_path=r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\SVM\\svm.pkl'\n",
    "    #print(\"Using model '{}'...\".format(model_name))\n",
    "    testing_images_labels_path = get_config('testing_images_labels_path')\n",
    "    with open(testing_images_labels_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        image_path, image_label = line.split()\n",
    "        frame = cv2.imread(image_path)\n",
    "        des=func(frame)\n",
    "        img_descs.append(des)\n",
    "        y.append(image_label)\n",
    "    y= vec_translate(y)\n",
    "    #finding indexes of test train and validate\n",
    "    y=np.array(y)\n",
    "    idxs=[]\n",
    "    for i in range(len(img_descs)):\n",
    "        idxs.append(i)\n",
    "    X, cluster_model = cluster_features(img_descs,idxs, MiniBatchKMeans(n_clusters=150))\n",
    "    \n",
    "    \n",
    "    total=len(img_descs)\n",
    "    cnt=0\n",
    "    classifier_model = joblib.load(model_serialized_path)\n",
    "    predicted_labels = classifier_model.predict(X)\n",
    "#     predicted_label = predicted_labels[0]\n",
    "    #print(predicted_labels)\n",
    "    for i in range(total):\n",
    "        if y[i] != predicted_labels[i]:\n",
    "                    cnt += 1\n",
    "    print(str(cnt)+\" \"+str(total))\n",
    "    print(\"Accuracy of SVM Classification on Test Data is: \"+str((total-cnt)/total))\n",
    "    cv2.destroyAllWindows()\n",
    "    #print (\"The program completed successfully !!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from common.config import get_config\n",
    "from common.image_transformation import apply_image_transformation\n",
    "import warnings\n",
    "\n",
    "def main():\n",
    "    label=0\n",
    "    img_descs=[]\n",
    "    y=[]\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model_serialized_path=r'C:\\Users\\Subham\\Documents\\Indian-Sign-Language\\data\\generated\\output(SURF)\\MLP\\mlp.pkl'\n",
    "    #print(\"Using model '{}'...\".format(model_name))\n",
    "    testing_images_labels_path = get_config('testing_images_labels_path')\n",
    "    with open(testing_images_labels_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        image_path, image_label = line.split()\n",
    "        frame = cv2.imread(image_path)\n",
    "        des=func(frame)\n",
    "        img_descs.append(des)\n",
    "        y.append(image_label)\n",
    "    y= vec_translate(y)\n",
    "    #finding indexes of test train and validate\n",
    "    y=np.array(y)\n",
    "    idxs=[]\n",
    "    for i in range(len(img_descs)):\n",
    "        idxs.append(i)\n",
    "    X, cluster_model = cluster_features(img_descs,idxs, MiniBatchKMeans(n_clusters=150))\n",
    "    \n",
    "    \n",
    "    total=len(img_descs)\n",
    "    cnt=0\n",
    "    classifier_model = joblib.load(model_serialized_path)\n",
    "    predicted_labels = classifier_model.predict(X)\n",
    "#     predicted_label = predicted_labels[0]\n",
    "    #print(predicted_labels)\n",
    "    for i in range(total):\n",
    "        if y[i] != predicted_labels[i]:\n",
    "                    cnt += 1\n",
    "    print(str(cnt)+\" \"+str(total))\n",
    "    print(\"Accuracy of MLP Classification on Test Data is: \"+str((total-cnt)/total))\n",
    "    cv2.destroyAllWindows()\n",
    "    #print (\"The program completed successfully !!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
